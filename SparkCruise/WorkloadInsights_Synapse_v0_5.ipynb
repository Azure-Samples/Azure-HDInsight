{
  "metadata": {
    "saveOutput": true,
    "kernelspec": {
      "name": "pyspark3kernel",
      "display_name": "PySpark3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_status": {
          "execute_time": {
            "duration": 147.34814453125,
            "end_time": 1584417525073.1
          }
        },
        "editable": true
      },
      "source": [
        "# Workload Insights Notebook\n",
        "\n",
        "This notebook analyzes the denormalized representation of query workload. This denormalized dataset contains the information from applications, queries, plans, and runtime metrics. The plans were also annotated with signatures to identify subexpressions. We hope that the analysis in the notebook helps you in making data-driven decisions.\n",
        "\n",
        "\n",
        "## Features\n",
        "1. Sample Rows\n",
        "3. Query Count\n",
        "4. Operator Frequencies\n",
        "5. Overlapping Queries\n",
        "6. Common Subexpressions Per Operator\n",
        "7. Selected Views\n",
        "8. SparkCruise Savings\n",
        "9. Filter Selectivity\n",
        "10. Exchange Operator Statistics\n",
        "11. Recurring Jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cell_status": {
          "execute_time": {
            "duration": 5309.25,
            "end_time": 1591330779056.231
          }
        },
        "editable": true,
        "inputCollapsed": true,
        "collapsed": false
      },
      "source": [
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.embed import file_html\n",
        "from bokeh.resources import CDN\n",
        "import pandas as pd\n",
        "\n",
        "artifactDir = \"/peregrine/\"\n",
        "schema = \"AppID: string, AppName: string, NormAppName: string, UserName: string, ClusterName: string, Subscription: string, AppSubmitTime: long, AppStartTime: long, AppEndTime: long, AppWallClockTime: long, QueryID: long, AppQueryID: string, QueryStartTime: long, QueryEndTime: long, QueryWallClockTime: long, OperatorName: string, OperatorID: long, TreeLevel: long, ParentID: long, ChildCount: long, LogicalName: string, StrictSignature: string, NonStrictSignature: string, PRowCount: long, PExclusiveTime: long, PSerialTime: long, Parameters: string, Bytes: long, RowCount: long, ExclusiveTime: long, MaxMemory: long, InputCard: long, AvgRowLength: long, InputDataset: string\"\n",
        "logicalExps = spark.read.format(\"csv\").option(\"sep\", \"|\").option(\"schema\", schema).option(\"header\", \"true\").load(artifactDir + \"/logical_ir\")\n",
        "logicalExps.createOrReplaceTempView(\"LogicalExps\")\n",
        "physicalExps = spark.read.format(\"csv\").option(\"sep\", \"|\").option(\"schema\", schema).option(\"header\", \"true\").load(artifactDir + \"/physical_ir\")\n",
        "physicalExps.createOrReplaceTempView(\"PhysicalExps\")\n",
        "analysisExps = spark.sql(\"SELECT * FROM LogicalExps WHERE ChildCount > 0\")\n",
        "analysisExps.createOrReplaceTempView(\"AnalysisExps\")\n",
        "repeatSubexps = spark.sql(\"SELECT StrictSignature FROM AnalysisExps GROUP BY StrictSignature HAVING COUNT(DISTINCT AppQueryID) > 1\")\n",
        "repeatSubexps.createOrReplaceTempView(\"RepeatSubexps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Rows\n",
        "Example of records in intermediate representation.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [],
            "yLabel": "",
            "xLabel": "",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "sampleQuery = \"\"\"\n",
        "SELECT AppID, AppName, AppStartTime, QueryID, QueryWallClockTime, OperatorName, LogicalName, StrictSignature, PRowCount, PExclusiveTime\n",
        "FROM PhysicalExps \n",
        "WHERE LENGTH(StrictSignature) > 0 AND PExclusiveTime > 0 \n",
        "ORDER BY rand() \n",
        "LIMIT 10\"\"\"\n",
        "sample  = spark.sql(sampleQuery)\n",
        "display(sample)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true
      },
      "source": [
        "## Query and Operator Count\n",
        "\n",
        "Number of queries and operators in workload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cell_status": {
          "execute_time": {
            "duration": 7332.1279296875,
            "end_time": 1591330786402.865
          }
        },
        "editable": true,
        "inputCollapsed": true,
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [
              "DistinctOperators"
            ],
            "yLabel": "DistinctOperators",
            "xLabel": "",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"DistinctOperators\":{\"\":8}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "queryOpCountQuery = \"\"\"\n",
        "SELECT COUNT(DISTINCT AppQueryID) AS QueryCount, COUNT(*) AS OperatorCount, COUNT(DISTINCT OperatorName) AS DistinctOperators\n",
        "FROM AnalysisExps\"\"\"\n",
        "totalQueryOpCount  = spark.sql(queryOpCountQuery)\n",
        "display(totalQueryOpCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operator Frequency\n",
        "Frequency of logical and physical operators in workload.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "pie",
            "keys": [
              "OperatorName"
            ],
            "values": [
              "Frequency"
            ],
            "yLabel": "Frequency",
            "xLabel": "OperatorName",
            "aggregation": "SUM",
            "aggByBackend": false,
            "isValid": true,
            "inValidMsg": null,
            "series": null
          },
          "aggData": "{\"Frequency\":{\"Aggregate\":1,\"AnalyzeTableCommand\":2,\"CreateDataSourceTableAsSelectCommand\":2,\"CreateViewCommand\":1,\"DropTableCommand\":2,\"Filter\":11,\"GlobalLimit\":7,\"Join\":4,\"LocalLimit\":7,\"LogicalRDD\":2,\"LogicalRelation\":12,\"Project\":12,\"Sort\":1}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "logicalOpFreqQuery = \"\"\"\n",
        "SELECT OperatorName, COUNT(*) AS Frequency\n",
        "FROM LogicalExps\n",
        "GROUP BY OperatorName\n",
        "ORDER BY Frequency DESC\"\"\"\n",
        "logicalOpFreq  = spark.sql(logicalOpFreqQuery)\n",
        "display(logicalOpFreq)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frequency of physical operators in workload.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "pie",
            "keys": [
              "OperatorName"
            ],
            "values": [
              "Frequency"
            ],
            "yLabel": "Frequency",
            "xLabel": "OperatorName",
            "aggregation": "SUM",
            "aggByBackend": false,
            "isValid": true,
            "inValidMsg": null
          },
          "aggData": "{\"Frequency\":{\"BroadcastExchangeExec\":4,\"BroadcastHashJoinExec\":4,\"CollectLimitExec\":6,\"DataWritingCommandExec\":2,\"ExecutedCommandExec\":5,\"FileSourceScanExec\":12,\"FilterExec\":11,\"HashAggregateExec\":2,\"InputAdapter\":5,\"ProjectExec\":13,\"RDDScanExec\":2,\"ShuffleExchangeExec\":1,\"TakeOrderedAndProjectExec\":1,\"WholeStageCodegenExec\":13}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "physicalOpFreqQuery = \"\"\"\n",
        "SELECT OperatorName, COUNT(*) AS Frequency\n",
        "FROM PhysicalExps\n",
        "GROUP BY OperatorName\n",
        "ORDER BY Frequency DESC\"\"\"\n",
        "physicalOpFreq  = spark.sql(physicalOpFreqQuery)\n",
        "display(physicalOpFreq)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true
      },
      "source": [
        "### Overlapping queries\n",
        "Queries with overlapping computations.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cell_status": {
          "execute_time": {
            "duration": 2267.808837890625,
            "end_time": 1591330788684.089
          }
        },
        "editable": true,
        "inputCollapsed": true,
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "line",
            "keys": [
              "QueryCount",
              "QueriesWithOneOverlap",
              "QueriesWithTwoOverlaps"
            ],
            "values": [],
            "yLabel": "",
            "xLabel": "QueryCount,QueriesWithOneOverlap,QueriesWithTwoOverlaps",
            "aggregation": "SUM",
            "aggByBackend": false,
            "isValid": false,
            "inValidMsg": "At least one value column is required!"
          },
          "aggData": "{}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "queryCountQuery = \"\"\"\n",
        "SELECT COUNT(DISTINCT AppQueryID) AS QueryCount \n",
        "FROM AnalysisExps\"\"\"\n",
        "queriesWithRepeatQuery = \"\"\"\n",
        "SELECT COUNT(DISTINCT AppQueryID) AS QueriesWithOneOverlap\n",
        "FROM AnalysisExps\n",
        "WHERE StrictSignature IN \n",
        "( SELECT StrictSignature\n",
        "FROM RepeatSubexps )\"\"\"\n",
        "queriesWithTwoRepeatsQuery = \"\"\"\n",
        "SELECT COUNT(*) AS QueriesWithTwoOverlaps FROM (\n",
        "SELECT AppQueryID, COUNT(*) AS Repeats \n",
        "FROM AnalysisExps\n",
        "WHERE StrictSignature IN \n",
        "( SELECT StrictSignature\n",
        "FROM RepeatSubexps )\n",
        "GROUP BY AppQueryID\n",
        "HAVING Repeats > 1\n",
        "ORDER BY Repeats DESC)\"\"\"\n",
        "allQueryOverlapQuery = \"\"\"\n",
        "SELECT *, CAST((QueriesWithOneOverlap/QueryCount)*100 AS Decimal(38,2)) AS OverlapPercent \n",
        "FROM ({}) AS R1, ({}) AS R2, ({}) AS R3 \"\"\".format(queryCountQuery, queriesWithRepeatQuery, queriesWithTwoRepeatsQuery)\n",
        "queryOverlap  = spark.sql(allQueryOverlapQuery)\n",
        "display(queryOverlap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overlapping Computations\n",
        "\n",
        "Overlapping computations per operator.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "OperatorName"
            ],
            "values": [
              "Total",
              "Repeats"
            ],
            "yLabel": "Total,Repeats",
            "xLabel": "OperatorName",
            "aggregation": "SUM",
            "aggByBackend": false,
            "isValid": true,
            "inValidMsg": null
          },
          "aggData": "{\"Total\":{\"CreateDataSourceTableAsSelectCommand\":2,\"Filter\":11,\"GlobalLimit\":7,\"Join\":4,\"LocalLimit\":7,\"Project\":12},\"Repeats\":{\"CreateDataSourceTableAsSelectCommand\":2,\"Filter\":10,\"GlobalLimit\":2,\"Join\":4,\"LocalLimit\":2,\"Project\":8}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "opFreqQuery = \"\"\"\n",
        "SELECT OperatorName, COUNT(*) AS Total\n",
        "FROM AnalysisExps\n",
        "GROUP BY OperatorName\n",
        "ORDER BY Total DESC\"\"\"\n",
        "opRepeatSubexpQuery = \"\"\"\n",
        "SELECT OperatorName, COUNT(*) AS Repeats\n",
        "FROM AnalysisExps\n",
        "WHERE StrictSignature IN \n",
        "( SELECT StrictSignature\n",
        "FROM RepeatSubexps )\n",
        "GROUP BY OperatorName\n",
        "ORDER BY Repeats DESC\"\"\"\n",
        "opDistinctRepeatQuery = \"\"\"\n",
        "SELECT OperatorName, COUNT(*) AS DistinctRepeats\n",
        "FROM ( SELECT DISTINCT OperatorName, StrictSignature \n",
        "FROM AnalysisExps\n",
        "WHERE StrictSignature IN \n",
        "( SELECT StrictSignature\n",
        "FROM RepeatSubexps ))\n",
        "GROUP BY OperatorName\n",
        "ORDER BY DistinctRepeats DESC\"\"\"\n",
        "opAvgRepeatsQuery = \"\"\"\n",
        "SELECT R1.OperatorName, Total, Repeats, DistinctRepeats, CAST(Repeats/DistinctRepeats AS Decimal(38,2)) AS AvgRepFrequency, CAST((Repeats/Total)*100 AS Decimal(38,2)) AS RepeatPercent  \n",
        "FROM ({}) AS R1, ({}) AS R2, ({}) AS R3\n",
        "WHERE R1.OperatorName = R2.OperatorName AND R2.OperatorName = R3.OperatorName\n",
        "ORDER BY RepeatPercent DESC\"\"\".format(opFreqQuery, opRepeatSubexpQuery, opDistinctRepeatQuery)\n",
        "opAvgRepeats = spark.sql(opAvgRepeatsQuery)\n",
        "display(opAvgRepeats)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selected Views\n",
        "Per operator summary of selected views."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true
      },
      "source": [
        "selViews = spark.read.format(\"csv\").option(\"sep\", \"|\").option(\"schema\", schema).option(\"header\", \"true\").load(artifactDir + \"views_ir\")\n",
        "selViews.createOrReplaceTempView(\"SelViews\")\n",
        "views = spark.sql(\"SELECT DISTINCT StrictSignature FROM SelViews\")\n",
        "views.createOrReplaceTempView(\"Views\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "LogicalName"
            ],
            "values": [
              "ViewCount"
            ],
            "yLabel": "ViewCount",
            "xLabel": "LogicalName",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"ViewCount\":{\"Filter\":1,\"Join\":2}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "distinctViewsQuery = \"\"\"\n",
        "SELECT P.LogicalName AS LogicalName, COUNT(DISTINCT P.StrictSignature) AS ViewCount\n",
        "FROM Views V, PhysicalExps P\n",
        "WHERE V.StrictSignature = P.StrictSignature \n",
        "GROUP BY P.LogicalName\n",
        "ORDER BY ViewCount DESC\"\"\"\n",
        "\n",
        "viewSubexprsOpsQuery = \"\"\"\n",
        "SELECT P.LogicalName AS LogicalName, COUNT(*) AS ViewRepeats\n",
        "FROM Views V, PhysicalExps P\n",
        "WHERE V.StrictSignature = P.StrictSignature \n",
        "GROUP BY P.LogicalName\n",
        "ORDER BY ViewRepeats DESC\"\"\"\n",
        "\n",
        "joinViewQuery = \"\"\"\n",
        "SELECT X.LogicalName, ViewCount, ViewRepeats\n",
        "FROM ({}) AS X, ({}) AS Y\n",
        "WHERE X.LogicalName = Y.LogicalName\"\"\".format(distinctViewsQuery,viewSubexprsOpsQuery)\n",
        "\n",
        "joinView = spark.sql(joinViewQuery)\n",
        "display(joinView)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SparkCruise Savings\n",
        "Potential savings per view.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "LogicalName"
            ],
            "values": [
              "NumOccurrences"
            ],
            "yLabel": "NumOccurrences",
            "xLabel": "LogicalName",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"NumOccurrences\":{\"Filter\":2,\"Join\":4}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "avgBenefitsQuery = \"\"\"\n",
        "SELECT LogicalName, NumOccurrences, AvgSerialTime_ms, AvgRowCount, AvgRowLength_bytes\n",
        "FROM (\n",
        "SELECT P.LogicalName AS LogicalName, P.StrictSignature AS Id, COUNT(*) AS NumOccurrences, AVG(PSerialTime) AS AvgSerialTime_ms, AVG(PRowCount) AS AvgRowCount, AVG(AvgRowLength) AS AvgRowLength_bytes\n",
        "FROM Views V, PhysicalExps P\n",
        "WHERE V.StrictSignature = P.StrictSignature\n",
        "GROUP BY P.LogicalName, P.StrictSignature)\n",
        "\"\"\"\n",
        "\n",
        "avgBenefits = spark.sql(avgBenefitsQuery)\n",
        "display(avgBenefits)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter Selectivity\n",
        "Selectivity of filters.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "AppQueryID"
            ],
            "values": [
              "FilterSel"
            ],
            "yLabel": "FilterSel",
            "xLabel": "AppQueryID",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"FilterSel\":{\"application_1599386412877_0001_10\":2,\"application_1599386412877_0001_11\":1,\"application_1599386412877_0001_12\":1,\"application_1599386412877_0001_6\":0.2,\"application_1599386412877_0001_7\":0.4,\"application_1599386412877_0001_8\":0.4,\"application_1599386412877_0001_9\":2}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "filterSelQuery = \"\"\"\n",
        "SELECT F.AppQueryID, F.OperatorName AS FilterOp, F.RowCount AS PassCount, T.OperatorName AS ScanOp, F.InputCard AS TotalCount, CAST(F.RowCount/F.InputCard AS Decimal(38, 5)) AS FilterSel\n",
        "FROM PhysicalExps F, PhysicalExps T \n",
        "WHERE F.AppQueryID = T.AppQueryID AND\n",
        "F.OperatorName = 'FilterExec' AND \n",
        "T.ParentID = F.OperatorID AND\n",
        "T.RowCount > 0 AND \n",
        "T.ChildCount = 0\n",
        "ORDER BY FilterSel\"\"\"\n",
        "\n",
        "filterSel = spark.sql(filterSelQuery).toPandas()\n",
        "filterSel['pdf'] = filterSel['FilterSel']/sum(filterSel['FilterSel'])\n",
        "filterSel['ecdf'] = (filterSel['pdf'].cumsum())\n",
        "\n",
        "p = figure(plot_width=400, plot_height=400, title=\"Filter Selectivity\")\n",
        "p.line(filterSel['FilterSel'], filterSel['ecdf'], line_width=2)\n",
        "p.xaxis.axis_label = \"Filter Selectivity\"\n",
        "p.yaxis.axis_label = \"CDF\"\n",
        "html = file_html(p, CDN, \"Filter Selectivity\")\n",
        "displayHTML(html)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exchange\n",
        "How many rows are shuffled in real-world Spark workloads?"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 2,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "RowCountBucket"
            ],
            "values": [
              "Count"
            ],
            "yLabel": "Count",
            "xLabel": "RowCountBucket",
            "aggregation": "SUM",
            "aggByBackend": false,
            "isValid": true,
            "inValidMsg": null
          },
          "aggData": "{\"Count\":{\"2. 1-1k\":1}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "def getBucket(rowCount):\n",
        "    rows = int(rowCount)\n",
        "    if rows <=1:\n",
        "        return \"1. 0-1\"\n",
        "    elif (rows > 1 and rows <= 1e3):\n",
        "        return \"2. 1-1k\"\n",
        "    elif (rows > 1e3 and rows <= 1e5):\n",
        "        return \"3. 1k-100k\"\n",
        "    elif (rows > 1e5 and rows <= 1e6):\n",
        "        return \"4. 100k-1M\"\n",
        "    elif (rows > 1e6 and rows <= 1e8):\n",
        "        return \"5. 1M-100M\"\n",
        "    elif (rows > 1e8 and rows <=  1e9):\n",
        "        return \"6. 100M-1B\"\n",
        "    else:\n",
        "        return \"7. >1B\"\n",
        "\n",
        "spark.udf.register(\"getBucket\", getBucket)\n",
        "exBucketQuery = \"\"\"\n",
        "SELECT Bucket AS RowCountBucket, CAST(AVG(MB) AS Decimal(38,2)) AS AvgDataSizeInMB, CAST(MAX(MB) AS Decimal(38,2)) AS MaxDataSizeInMB, COUNT(*) AS Count\n",
        "FROM (\n",
        "SELECT getBucket(PRowCount) AS Bucket, Bytes/(1024.0*1024) AS MB\n",
        "FROM PhysicalExps\n",
        "WHERE PRowCount > 0 AND Bytes > 0 AND OperatorName LIKE '%ShuffleExchangeExec%')\n",
        "GROUP BY Bucket\n",
        "ORDER BY RowCountBucket\"\"\"\n",
        "exBucket = spark.sql(exBucketQuery)\n",
        "display(exBucket)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recurring jobs\n",
        "Recurring jobs share same non-strict signature at the root level. To capture temporal patterns, we can take the intersection of the result set for a few consecutive days and then split the intesection set in hourly, daily repeat patterns based on the AppSubmitTime value."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "inputCollapsed": true,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "Subscription"
            ],
            "values": [
              "Count"
            ],
            "yLabel": "Count",
            "xLabel": "Subscription",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"Count\":{\"\":4}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "recurringJobsQuery = \"\"\"\n",
        "SELECT Subscription, NonStrictSignature, COUNT(*) AS Count\n",
        "FROM AnalysisExps\n",
        "WHERE TreeLevel = 0 AND LENGTH(NonStrictSignature) > 0\n",
        "GROUP BY Subscription, NonStrictSignature\n",
        "HAVING COUNT(*) > 1\n",
        "ORDER BY Count DESC\"\"\"\n",
        "recurringJobs = spark.sql(recurringJobsQuery)\n",
        "display(recurringJobs)"
      ],
      "attachments": {}
    }
  ]
}